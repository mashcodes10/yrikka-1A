# -*- coding: utf-8 -*-
"""newYrikkaProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ytkSJUhHvuH4bXPVo8x3px4jTda9H4k6
"""

!git clone https://github.com/YRIKKA/yrikka-btt-aistudio-2025.git

import os
import json
import csv
import shutil
from tqdm import tqdm
from PIL import Image

def clean_dataset(images_dir, coco_json_path, out_dir):

    # Create output directory & images folder
    os.makedirs(os.path.join(out_dir, "images"), exist_ok=True)

    # -------------------------------------------
    # Step 1: Define canonical classes + mappings
    # -------------------------------------------
    canonical_classes = ["potted plant", "chair", "cup", "vase", "book"]
    class_mapping = {
        "##ted": "potted plant",
        "pot plant": "potted plant",
        "cup vase": "vase",
        "pot": "potted plant",
        "vase potted plant": "potted plant",
        "potted": "potted plant"
    }

    # -------------------------------------------
    # Step 2: Load COCO annotations
    # -------------------------------------------
    with open(coco_json_path) as f:
        coco = json.load(f)

    # Map canonical class names ‚Üí new IDs (1 to N)
    name_to_new_id = {cls: i + 1 for i, cls in enumerate(canonical_classes)}

    # Original COCO category_id ‚Üí category_name mapping
    category_id_to_name = {cat["id"]: cat["name"] for cat in coco.get("categories", [])}

    # -------------------------------------------
    # Step 3: Collect valid images + dimensions
    # -------------------------------------------
    image_id_to_info = {}
    for img in coco["images"]:
        file_path = os.path.join(images_dir, img["file_name"])
        if not os.path.exists(file_path):
            continue
        try:
            with Image.open(file_path) as im:
                width, height = im.size
        except:
            continue

        image_id_to_info[img["id"]] = {
            "file_name": img["file_name"],
            "path": file_path,
            "width": width,
            "height": height
        }

    # -------------------------------------------
    # Step 4: Validate and clean annotations
    # -------------------------------------------
    valid_annotations = []
    valid_image_ids = set()
    dropped_rows = []
    used_original_classes = []  # Track only class names actually present

    for ann in tqdm(coco["annotations"], desc="Validating annotations"):
        image_id = ann["image_id"]

        # Skip annotation if image is missing/unloadable
        if image_id not in image_id_to_info:
            dropped_rows.append(["Missing image", image_id, ann["id"], ann.get("bbox", "")])
            continue

        # Validate bbox
        info = image_id_to_info[image_id]
        img_w, img_h = info["width"], info["height"]
        x, y, w, h = ann["bbox"]

        if w <= 0 or h <= 0:
            dropped_rows.append(["Invalid bbox size", image_id, ann["id"], ann["bbox"]])
            continue

        # Clip box to image boundaries
        x = max(0, min(x, img_w))
        y = max(0, min(y, img_h))
        w = max(1, min(w, img_w - x))
        h = max(1, min(h, img_h - y))

        if w <= 0 or h <= 0:
            dropped_rows.append(["Box outside image", image_id, ann["id"], [x, y, w, h]])
            continue

        # Apply class mapping for noisy labels
        original_class = category_id_to_name.get(ann["category_id"], "Unknown")
        if original_class not in used_original_classes:
          used_original_classes.append(original_class)  # Only track used classes
        mapped_class = class_mapping.get(original_class, original_class)

        # Keep only mapped or canonical classes
        if mapped_class not in canonical_classes:
            dropped_rows.append(["Unmapped class", image_id, ann["id"], original_class])
            continue

        # Final annotation cleanup
        ann["category_id"] = name_to_new_id[mapped_class]
        ann["bbox"] = [x, y, w, h]
        valid_annotations.append(ann)
        valid_image_ids.add(image_id)

    # -------------------------------------------
    # Step 5: Keep only valid images
    # -------------------------------------------
    valid_images = [img for img in coco["images"] if img["id"] in valid_image_ids]

    # -------------------------------------------
    # Step 6: Save cleaned COCO JSON
    # -------------------------------------------
    cleaned_coco = {
        "images": valid_images,
        "annotations": valid_annotations,
        "categories": [{"id": i + 1, "name": cls} for i, cls in enumerate(canonical_classes)]
    }
    with open(os.path.join(out_dir, "cleaned_coco.json"), "w") as f:
        json.dump(cleaned_coco, f, indent=2)

    # Copy only validated images to the new folder
    for img in valid_images:
        src = os.path.join(images_dir, img["file_name"])
        dst = os.path.join(out_dir, "images", img["file_name"])
        if os.path.exists(src):
            shutil.copy2(src, dst)

    # -------------------------------------------
    # Step 7: Save dropped annotation report
    # -------------------------------------------
    with open(os.path.join(out_dir, "dropped_report.csv"), "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Reason", "ImageID", "AnnotationID", "Details"])
        for r in dropped_rows:
            writer.writerow(r)

    # -------------------------------------------
    # Step 8: Save real class mapping used
    # -------------------------------------------
    with open(os.path.join(out_dir, "class_mapping.csv"), "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Original Class (found)", "Mapped Class"])
        for orig in used_original_classes:
            mapped = class_mapping.get(orig, orig)
            if orig != mapped:
                writer.writerow([orig, mapped])

    print(f"‚úÖ Cleaned dataset saved ({out_dir}): {len(valid_images)} images, {len(valid_annotations)} annotations")

# ---------------------------
# Run on both datasets
# ---------------------------
clean_dataset(
    images_dir="yrikka-btt-aistudio-2025/BTT_Data/852a64c6-4bd3-495f-8ff7-f5cc85e34316/images",
    coco_json_path="yrikka-btt-aistudio-2025/BTT_Data/852a64c6-4bd3-495f-8ff7-f5cc85e34316/coco.json",
    out_dir="cleaned_dataset1"
)

clean_dataset(
    images_dir="yrikka-btt-aistudio-2025/BTT_Data/8e0a5d2d-3ae0-4ff0-b6ee-2d85f7da4fee/images",
    coco_json_path="yrikka-btt-aistudio-2025/BTT_Data/8e0a5d2d-3ae0-4ff0-b6ee-2d85f7da4fee/coco.json",
    out_dir="cleaned_dataset2"
)

import os
import json
import shutil
from tqdm import tqdm

def merge_coco_datasets(cleaned_dir1, cleaned_dir2, output_dir):
    """
    Merges two cleaned COCO datasets (same categories) into one unified dataset.
    """

    os.makedirs(os.path.join(output_dir, "images"), exist_ok=True)

    # Load both COCO annotations
    with open(os.path.join(cleaned_dir1, "cleaned_coco.json")) as f:
        coco1 = json.load(f)
    with open(os.path.join(cleaned_dir2, "cleaned_coco.json")) as f:
        coco2 = json.load(f)

    # Categories should be the same (because we already cleaned them)
    categories = coco1["categories"]

    # To avoid ID conflicts, find maximum existing IDs in dataset1
    max_image_id = max([img["id"] for img in coco1["images"]], default=0)
    max_ann_id = max([ann["id"] for ann in coco1["annotations"]], default=0)

    merged_images = []
    merged_annotations = []

    # Step 1: Copy all images + annotations from dataset1
    for img in coco1["images"]:
        src = os.path.join(cleaned_dir1, "images", img["file_name"])
        dst = os.path.join(output_dir, "images", img["file_name"])
        if os.path.exists(src):
            shutil.copy2(src, dst)
        merged_images.append(img)

    for ann in coco1["annotations"]:
        merged_annotations.append(ann)

    # Step 2: Append all from dataset2 but shift IDs
    for img in coco2["images"]:
        new_img = img.copy()
        new_img["id"] = img["id"] + max_image_id  # shift image ID
        src = os.path.join(cleaned_dir2, "images", img["file_name"])
        dst = os.path.join(output_dir, "images", img["file_name"])
        if os.path.exists(src):
            shutil.copy2(src, dst)
        merged_images.append(new_img)

    for ann in coco2["annotations"]:
        new_ann = ann.copy()
        new_ann["id"] = ann["id"] + max_ann_id  # shift annotation ID
        new_ann["image_id"] = ann["image_id"] + max_image_id  # link to shifted image ID
        merged_annotations.append(new_ann)

    # Step 3: Save merged COCO JSON
    merged_coco = {
        "images": merged_images,
        "annotations": merged_annotations,
        "categories": categories
    }

    with open(os.path.join(output_dir, "merged_coco.json"), "w") as f:
        json.dump(merged_coco, f, indent=2)

    print(f"‚úÖ Merged dataset saved to {output_dir}:")
    print(f"   ‚Üí Total Images: {len(merged_images)}")
    print(f"   ‚Üí Total Annotations: {len(merged_annotations)}")


# ----------------------------
# Run After Your Cleaning Step
# ----------------------------
merge_coco_datasets(
    cleaned_dir1="cleaned_dataset1",
    cleaned_dir2="cleaned_dataset2",
    output_dir="merged_cleaned_dataset"
)

# from google.colab import files

# # Create a zip of the misclassified images folder
# shutil.make_archive("merged_cleaned_dataset/images", "zip", "merged_cleaned_dataset/images")


# # Download the zip

pip install ultralytics

from ultralytics import YOLO

import os
import shutil
import csv
import json
from collections import defaultdict
from ultralytics import YOLO

# -----------------------------
# Step 1. Run YOLO Predictions
# -----------------------------
model = YOLO("yolo11n.pt")

results = model.predict(
    source="merged_cleaned_dataset/images",
    save=True,
    save_txt=True,
    conf=0.5,
    classes=[58, 56, 41, 75, 73]  # COCO IDs for potted plant, chair, cup, vase, book
)

# -----------------------------
# Step 2. Class Mapping
# -----------------------------
coco_to_canonical = {58: 1, 56: 2, 41: 3, 75: 4, 73: 5}
canonical_names = {
    1: "potted plant", 2: "chair", 3: "cup", 4: "vase", 5: "book"
}

# -----------------------------
# Step 3. Load Ground Truth
# -----------------------------
with open("merged_cleaned_dataset/merged_coco.json") as f:
    gt = json.load(f)

image_id_to_ann = defaultdict(list)
for ann in gt["annotations"]:
    image_id_to_ann[ann["image_id"]].append(ann)

image_id_to_name = {img["id"]: img["file_name"] for img in gt["images"]}

# -----------------------------
# Step 4. IOU Function
# -----------------------------
def iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])
    inter = max(0, xB - xA) * max(0, yB - yA)
    union = boxA[2]*boxA[3] + boxB[2]*boxB[3] - inter
    return inter / union if union > 0 else 0

# -----------------------------
# Step 5. Compare Predictions vs GT
# -----------------------------
misclassified_images = set()
report_rows = []
iou_threshold = 0.4

for result in results:
    file_name = os.path.basename(result.path)

    # Match GT image id
    image_id = next((img["id"] for img in gt["images"] if img["file_name"] == file_name), None)
    if image_id is None:
        continue

    gt_boxes = [(ann["bbox"], ann["category_id"]) for ann in image_id_to_ann[image_id]]

    preds = []
    for box in result.boxes:
        coco_id = int(box.cls.cpu().numpy()[0])
        if coco_id in coco_to_canonical:
            pred_class = coco_to_canonical[coco_id]
            xyxy = box.xyxy.cpu().numpy()[0]
            w, h = xyxy[2] - xyxy[0], xyxy[3] - xyxy[1]
            preds.append(([float(xyxy[0]), float(xyxy[1]), float(w), float(h)], pred_class))

    matched_preds = set()

    # GT vs Preds
    for gt_box, gt_cls in gt_boxes:
        for i, (pred_box, pred_cls) in enumerate(preds):
            if i not in matched_preds and iou(gt_box, pred_box) >= iou_threshold:
                matched_preds.add(i)
                if pred_cls != gt_cls:
                    misclassified_images.add(file_name)
                    report_rows.append([file_name, "Wrong Class",
                                        canonical_names[gt_cls],
                                        canonical_names[pred_cls],
                                        round(iou(gt_box, pred_box), 3)])

    # Extra Predictions = Missing GT annotations
    for i, (pred_box, pred_cls) in enumerate(preds):
        if i not in matched_preds:
            misclassified_images.add(file_name)
            report_rows.append([file_name, "Missing Annotation", "None",
                                canonical_names[pred_cls], 0.0])

# -----------------------------
# Step 6. Save Everything in One Folder
# -----------------------------
output_folder = "misclassified_dataset"
images_folder = os.path.join(output_folder, "images")
os.makedirs(images_folder, exist_ok=True)

# Create COCO JSON with only misclassified images
misclassified_gt = {
    "images": [img for img in gt["images"] if img["file_name"] in misclassified_images],
    "annotations": [ann for ann in gt["annotations"]
                    if image_id_to_name[ann["image_id"]] in misclassified_images],
    "categories": gt["categories"]
}

with open(os.path.join(output_folder, "misclassified_coco.json"), "w") as f:
    json.dump(misclassified_gt, f, indent=2)

# Copy misclassified images
for img in misclassified_gt["images"]:
    src = os.path.join("merged_cleaned_dataset/images", img["file_name"])
    dst = os.path.join(images_folder, img["file_name"])
    if os.path.exists(src):
        shutil.copy2(src, dst)

# Save CSV Report
with open(os.path.join(output_folder, "misclassified_report.csv"), "w", newline="") as f:
    writer = csv.writer(f)
    writer.writerow(["Image", "Reason", "GroundTruth", "Prediction", "IOU"])
    writer.writerows(report_rows)

print(f"‚úÖ {len(misclassified_gt['images'])} misclassified images found.")
print(f"üìÅ All outputs saved inside '{output_folder}/'")

import csv

wrong_class = 0
missing_annotation = 0

with open("misclassified_dataset/misclassified_report.csv", newline="") as f:
    reader = csv.reader(f)
    next(reader)  # skip header
    for row in reader:
        reason = row[1]
        if reason == "Wrong Class":
            wrong_class += 1
        elif reason == "Missing Annotation":
            missing_annotation += 1

total = wrong_class + missing_annotation

print("Wrong Class:", wrong_class)
print("Missing Annotation:", missing_annotation)
print("Total Flagged:", total)

import json

with open("misclassified_dataset/misclassified_coco.json") as f:
    coco = json.load(f)

total_annotations = len(coco["annotations"])
print("Total annotations in misclassified COCO JSON:", total_annotations)

#-----------------------------
# Merging the cvat export with original merged dataset
# upload merged_cvat_all.json before running this cell.
#-----------------------------

import json, copy, hashlib
import math

# ------------------------------
# Helper: IoU for two COCO bboxes
# bbox = [x, y, w, h]
# ------------------------------
def iou(b1, b2):
    x1, y1, w1, h1 = b1
    x2, y2, w2, h2 = b2

    xa = max(x1, x2)
    ya = max(y1, y2)
    xb = min(x1 + w1, x2 + w2)
    yb = min(y1 + h1, y2 + h2)

    inter_w = xb - xa
    inter_h = yb - ya
    if inter_w <= 0 or inter_h <= 0:
        return 0.0

    inter_area = inter_w * inter_h
    area1 = w1 * h1
    area2 = w2 * h2
    union_area = area1 + area2 - inter_area

    return inter_area / union_area if union_area > 0 else 0.0


# ------------------------------
# Helper: Hash annotation to detect exact duplicates
# ------------------------------
def hash_ann(a):
    txt = json.dumps({
        "category_id": a.get("category_id"),
        "bbox": a.get("bbox"),
        "segmentation": a.get("segmentation"),
        "area": a.get("area"),
        "iscrowd": a.get("iscrowd")
    }, sort_keys=True)
    return hashlib.md5(txt.encode()).hexdigest()


# ------------------------------
# Helper: category name ‚Üí object
# ------------------------------
def index_by_name(cats):
    return {c["name"]: c for c in cats}


# ------------------------------
# Main function
# ------------------------------
def fix_and_merge(all_path, corrected_path, out_path, verbose=True,
                  iou_duplicate_threshold=0.90):

    # load both files
    with open(all_path, "r") as f:
        all_coco = json.load(f)
    with open(corrected_path, "r") as f:
        corr_coco = json.load(f)

    # final merged structure
    final = {
        "images": [],
        "annotations": [],
        "categories": [],
        "info": all_coco.get("info", {}),
        "licenses": all_coco.get("licenses", [])
    }

    # --------------------------------------------------------------------
    # STEP 1 ‚Äî Merge categories (by name)
    # --------------------------------------------------------------------
    cats1 = all_coco.get("categories", [])
    cats2 = corr_coco.get("categories", [])

    final["categories"] = copy.deepcopy(cats1)
    name_to_cat = index_by_name(cats1)
    next_cat_id = max([c["id"] for c in cats1], default=0) + 1

    cat_map = {}

    for c in cats2:
        name = c["name"]
        if name in name_to_cat:
            cat_map[c["id"]] = name_to_cat[name]["id"]
        else:
            new_c = copy.deepcopy(c)
            new_c["id"] = next_cat_id
            final["categories"].append(new_c)
            name_to_cat[name] = new_c
            cat_map[c["id"]] = next_cat_id
            next_cat_id += 1

    # --------------------------------------------------------------------
    # STEP 2 ‚Äî Merge images by file_name
    # --------------------------------------------------------------------
    img1 = all_coco.get("images", [])
    img2 = corr_coco.get("images", [])

    final["images"] = copy.deepcopy(img1)
    fname_to_id = {img["file_name"]: img["id"] for img in img1}
    next_img_id = max([img["id"] for img in img1], default=0) + 1

    img_map = {}

    for img in img2:
        fn = img["file_name"]
        if fn in fname_to_id:
            img_map[img["id"]] = fname_to_id[fn]
        else:
            new_img = copy.deepcopy(img)
            new_img["id"] = next_img_id
            final["images"].append(new_img)
            fname_to_id[fn] = next_img_id
            img_map[img["id"]] = next_img_id
            next_img_id += 1

    # --------------------------------------------------------------------
    # STEP 3 ‚Äî Remove wrong annotations
    # --------------------------------------------------------------------
    corrected_filenames = {img["file_name"] for img in img2}
    corrected_image_ids = {
        fname_to_id[f] for f in corrected_filenames if f in fname_to_id
    }

    all_ann = all_coco.get("annotations", [])
    cleaned_all_ann = [
        ann for ann in all_ann
        if ann["image_id"] not in corrected_image_ids
    ]

    # --------------------------------------------------------------------
    # STEP 4 ‚Äî Add corrected annotations (with ID remap)
    # --------------------------------------------------------------------
    final["annotations"] = cleaned_all_ann
    next_ann_id = max([a["id"] for a in cleaned_all_ann], default=0) + 1

    # store annotations by (image, category) for IoU dup detection
    ann_index = {}
    for a in cleaned_all_ann:
        key = (a["image_id"], a["category_id"])
        ann_index.setdefault(key, []).append(a)

    ann_hashes = {hash_ann(a) for a in cleaned_all_ann}

    added = 0
    skipped_iou = 0

    for ann in corr_coco.get("annotations", []):
        new_ann = copy.deepcopy(ann)
        new_ann["id"] = next_ann_id
        new_ann["image_id"] = img_map[new_ann["image_id"]]
        new_ann["category_id"] = cat_map[new_ann["category_id"]]

        key = (new_ann["image_id"], new_ann["category_id"])

        # ---- Step A: Exact duplicate detection
        h = hash_ann(new_ann)
        if h in ann_hashes:
            continue

        # ---- Step B: Near-duplicate IoU filtering
        existing_list = ann_index.get(key, [])
        is_duplicate = False
        for ex in existing_list:
            if iou(ex["bbox"], new_ann["bbox"]) > iou_duplicate_threshold:
                is_duplicate = True
                skipped_iou += 1
                break

        if is_duplicate:
            continue

        # add annotation
        ann_hashes.add(h)
        final["annotations"].append(new_ann)
        ann_index.setdefault(key, []).append(new_ann)
        next_ann_id += 1
        added += 1

    print(f"Added corrected annotations: {added}")
    print(f"Skipped near-duplicate boxes (IoU): {skipped_iou}")

    # --------------------------------------------------------------------
    # Save
    # --------------------------------------------------------------------
    with open(out_path, "w") as f:
        json.dump(final, f, indent=2)

    print("‚úÖ Merged JSON saved as", out_path)

all_json_path = "merged_cleaned_dataset/merged_coco.json"
corrected_json_path = "merged_cvat_all.json"
output_path = "cleaned_merged_coco.json"

fix_and_merge(all_json_path, corrected_json_path, output_path, verbose=True)

print("Done!")

import json

# Load the COCO JSON file
with open("cleaned_merged_coco.json") as f:  # replace with your JSON file path
    coco = json.load(f)

# Count total annotations
total_annotations = len(coco["annotations"])
print("Total annotations in COCO JSON:", total_annotations)

# count total images too
total_images = len(coco["images"])
print("Total images in COCO JSON:", total_images)

import os
import shutil
import csv
import json
from collections import defaultdict
from ultralytics import YOLO

# -----------------------------
# Paths
# -----------------------------
images_input_folder = "merged_cleaned_dataset/images"
coco_json_path = "cleaned_merged_coco.json"
output_folder = "new_misclassified_dataset"  # single output folder
images_output_folder = os.path.join(output_folder, "images")
os.makedirs(images_output_folder, exist_ok=True)

# -----------------------------
# Step 1. Run YOLO Predictions
# -----------------------------
model = YOLO("yolo11n.pt")

results = model.predict(
    source=images_input_folder,
    save=True,
    conf=0.5,
    classes=[58, 56, 41, 75, 73]  # COCO IDs for potted plant, chair, cup, vase, book
)

# -----------------------------
# Step 2. Class Mapping
# -----------------------------
coco_to_canonical = {58: 1, 56: 2, 41: 3, 75: 4, 73: 5}
canonical_names = {
    1: "potted plant", 2: "chair", 3: "cup", 4: "vase", 5: "book"
}

# -----------------------------
# Step 3. Load Ground Truth
# -----------------------------
with open(coco_json_path) as f:
    gt = json.load(f)

image_id_to_ann = defaultdict(list)
for ann in gt["annotations"]:
    image_id_to_ann[ann["image_id"]].append(ann)

image_id_to_name = {img["id"]: img["file_name"] for img in gt["images"]}

# -----------------------------
# Step 4. IOU Function
# -----------------------------
def iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])
    inter = max(0, xB - xA) * max(0, yB - yA)
    union = boxA[2]*boxA[3] + boxB[2]*boxB[3] - inter
    return inter / union if union > 0 else 0

# -----------------------------
# Step 5. Compare Predictions vs GT
# -----------------------------
misclassified_images = set()
report_rows = []
iou_threshold = 0.4

for result in results:
    file_name = os.path.basename(result.path)

    # Match GT image id
    image_id = next((img["id"] for img in gt["images"] if img["file_name"] == file_name), None)
    if image_id is None:
        continue

    gt_boxes = [(ann["bbox"], ann["category_id"]) for ann in image_id_to_ann[image_id]]

    preds = []
    for box in result.boxes:
        coco_id = int(box.cls.cpu().numpy()[0])
        if coco_id in coco_to_canonical:
            pred_class = coco_to_canonical[coco_id]
            xyxy = box.xyxy.cpu().numpy()[0]
            w, h = xyxy[2] - xyxy[0], xyxy[3] - xyxy[1]
            preds.append(([float(xyxy[0]), float(xyxy[1]), float(w), float(h)], pred_class))

    matched_preds = set()

    # GT vs Preds
    for gt_box, gt_cls in gt_boxes:
        for i, (pred_box, pred_cls) in enumerate(preds):
            if i not in matched_preds and iou(gt_box, pred_box) >= iou_threshold:
                matched_preds.add(i)
                if pred_cls != gt_cls:
                    misclassified_images.add(file_name)
                    report_rows.append([file_name, "Wrong Class",
                                        canonical_names[gt_cls],
                                        canonical_names[pred_cls],
                                        round(iou(gt_box, pred_box), 3)])

    # Extra Predictions = Missing GT annotations
    for i, (pred_box, pred_cls) in enumerate(preds):
        if i not in matched_preds:
            misclassified_images.add(file_name)
            report_rows.append([file_name, "Missing Annotation", "None",
                                canonical_names[pred_cls], 0.0])

# -----------------------------
# Step 6. Create COCO JSON with misclassified images
# -----------------------------
misclassified_gt = {
    "images": [img for img in gt["images"] if img["file_name"] in misclassified_images],
    "annotations": [ann for ann in gt["annotations"]
                    if image_id_to_name[ann["image_id"]] in misclassified_images],
    "categories": gt["categories"]
}

with open(os.path.join(output_folder, "misclassified_coco.json"), "w") as f:
    json.dump(misclassified_gt, f, indent=2)

# -----------------------------
# Step 7. Copy misclassified images to new_dataset/images
# -----------------------------
for img in misclassified_gt["images"]:
    src = os.path.join(images_input_folder, img["file_name"])
    dst = os.path.join(images_output_folder, img["file_name"])
    if os.path.exists(src):
        shutil.copy2(src, dst)

# -----------------------------
# Step 8. Save CSV report
# -----------------------------
with open(os.path.join(output_folder, "misclassified_report.csv"), "w", newline="") as f:
    writer = csv.writer(f)
    writer.writerow(["Image", "Reason", "GroundTruth", "Prediction", "IOU"])
    writer.writerows(report_rows)

print(f"‚úÖ {len(misclassified_gt['images'])} misclassified images found.")
print(f"üìÅ All outputs saved inside '{output_folder}/'")

import csv

wrong_class = 0
missing_annotation = 0

with open("new_misclassified_dataset/misclassified_report.csv", newline="") as f:
    reader = csv.reader(f)
    next(reader)  # skip header
    for row in reader:
        reason = row[1]
        if reason == "Wrong Class":
            wrong_class += 1
        elif reason == "Missing Annotation":
            missing_annotation += 1

total = wrong_class + missing_annotation

print("Wrong Class:", wrong_class)
print("Missing Annotation:", missing_annotation)
print("Total Flagged:", total)

# this is done because when we run yolo model it creates a seperate
# predict/ folder in runs/detect/ folder every single time and can cause confusion

# The below code deletes the detect folder if we are running the yolo model for second time and so on.

!rm -rf runs/detect