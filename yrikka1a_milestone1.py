# -*- coding: utf-8 -*-
"""Yrikka1A_Milestone1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ol_AXYRt0tHfyBRLXxVdcymd2MHUR54b
"""

# Cloning the repo to access the dataset

!git clone https://github.com/YRIKKA/yrikka-btt-aistudio-2025.git

# Import all the necessary libraries

import os
import json
import shutil
import csv
import random
import yaml
from PIL import Image
from tqdm import tqdm
from shutil import copy2

# Part A: Load the dataset and clean the data to generate a validated, cleaned JSON file (cleaned_dataset)

images_dir = "yrikka-btt-aistudio-2025/BTT_Data/852a64c6-4bd3-495f-8ff7-f5cc85e34316/images"
coco_json_path = "yrikka-btt-aistudio-2025/BTT_Data/852a64c6-4bd3-495f-8ff7-f5cc85e34316/coco.json"
out_dir = "cleaned_dataset"
os.makedirs(os.path.join(out_dir, "images"), exist_ok=True)

# -----------------------------
# Canonical classes + mapping
# -----------------------------
canonical_classes = ["potted plant", "chair", "cup", "vase", "book"]
class_mapping = {
    "##ted": "potted plant",
    "pot plant": "potted plant",
    "cup vase": "vase",
    "pot": "potted plant",
    "vase potted plant": "potted plant"
}

# -----------------------------
# Load COCO JSON
# -----------------------------
with open(coco_json_path) as f:
    coco = json.load(f)

# Build category name â†’ new ID mapping
name_to_new_id = {cls: i for i, cls in enumerate(canonical_classes)}

# If original COCO categories exist, build lookup
category_id_to_name = {cat["id"]: cat["name"] for cat in coco.get("categories", [])}

# -----------------------------------------------------------------------------------------
# Create a dictionary storing image width and height for validating image boundaries later
# -----------------------------------------------------------------------------------------
image_id_to_info = {}
for img in coco["images"]:
    file_name = img["file_name"]
    file_path = os.path.join(images_dir, file_name)

    if not os.path.exists(file_path):
        print(f"Missing file: {file_name}")
        continue

    try:
        with Image.open(file_path) as im:
            width, height = im.size
    except Exception as e:
        print(f"Cannot open {file_name}: {e}")
        continue

    image_id_to_info[img["id"]] = {
        "file_name": file_name,
        "path": file_path,
        "width": width,
        "height": height
    }

# -----------------------------
# Validate the annotations
# -----------------------------
valid_annotations = []
valid_image_ids = set()
dropped_rows = []

for ann in tqdm(coco["annotations"], desc="Validating annotations"):
    image_id = ann["image_id"]

    # a) Make sure every annotation points to an existing image
    if image_id not in image_id_to_info:
        dropped_rows.append(["Missing image", image_id, ann["id"], ann.get("bbox", "")])
        continue

    # Retrieve image information (width and height) from the pre-built dictionary
    info = image_id_to_info[image_id]
    img_w, img_h = info["width"], info["height"]
    x, y, w, h = ann["bbox"]

    # b) Drop invalid bbox with zero or negative width/height
    if w <= 0 or h <= 0:
        dropped_rows.append(["Zero/negative box", image_id, ann["id"], ann["bbox"]])
        continue

    # c) Clip boxes to image bounds
    x = max(0, min(x, img_w))
    y = max(0, min(y, img_h))
    w = max(1, min(w, img_w - x))
    h = max(1, min(h, img_h - y))

    # Drop if box became invalid after clipping
    if w <= 0 or h <= 0:
        dropped_rows.append(["Box outside image after clipping", image_id, ann["id"], [x, y, w, h]])
        continue

    # d) Map noisy class names to 5 canonical classes
    class_name = category_id_to_name.get(ann["category_id"], "Unknown")
    mapped_class = class_mapping.get(class_name, class_name)
    if mapped_class not in canonical_classes:
        dropped_rows.append(["Unmapped class", image_id, ann["id"], class_name])
        continue

    # Reassign category ID to 0-4 (as YOLO requires 0-based class IDs)
    ann["category_id"] = name_to_new_id[mapped_class]
    ann["bbox"] = [x, y, w, h]

    valid_annotations.append(ann)
    valid_image_ids.add(image_id)

# -----------------------------
# Keep only valid images
# -----------------------------
valid_images = [img for img in coco["images"] if img["id"] in valid_image_ids]
dropped_images = [img for img in coco["images"] if img["id"] not in valid_image_ids]

# -----------------------------
# Build cleaned categories
# -----------------------------
cleaned_categories = [{"id": i, "name": cls} for i, cls in enumerate(canonical_classes)]

# -----------------------------
# Save cleaned COCO JSON
# -----------------------------
cleaned_coco = {
    "images": valid_images,
    "annotations": valid_annotations,
    "categories": cleaned_categories
}

with open(os.path.join(out_dir, "cleaned_coco.json"), "w") as f:
    json.dump(cleaned_coco, f, indent=2)

# -----------------------------
# Copy valid images
# -----------------------------
for img in valid_images:
    src = os.path.join(images_dir, img["file_name"])
    dst = os.path.join(out_dir, "images", img["file_name"])
    if os.path.exists(src):
        shutil.copy2(src, dst)

# --------------------------------------------------------------------------
# Saving CSV report to record how many invalid annotations were dropped
# --------------------------------------------------------------------------
report_path = os.path.join(out_dir, "dropped_report.csv")
with open(report_path, "w", newline="") as f:
    writer = csv.writer(f)
    writer.writerow(["Reason", "ImageID", "AnnotationID", "Details"])
    for row in dropped_rows:
        writer.writerow(row)
    # Report dropped images with no valid annotations
    for img in dropped_images:
        writer.writerow(["No valid annotations", img["id"], "", img["file_name"]])

print(f"Cleaned dataset saved: {len(valid_images)} images, {len(valid_annotations)} annotations")
print(f"Dropped {len(dropped_rows) + len(dropped_images)} items (see dropped_report.csv)")

# --------------------------------------------------------------------------
# Saving CSV report to record how source categories were mapped to canonical
# --------------------------------------------------------------------------
os.makedirs("cleaned_dataset", exist_ok=True)
csv_path = os.path.join("cleaned_dataset", "class_mapping.csv")

with open(csv_path, "w") as f:
    writer = csv.writer(f)
    writer.writerow(["Original Class", "Mapped Class"])
    for k, v in class_mapping.items():
        writer.writerow([k, v])

print(f"Category mapping CSV saved to {csv_path}")

#Part B: Converting Coco to YOLO format (yolo_dataset)

cleaned_dir = "cleaned_dataset"
yolo_dir = "yolo_dataset"
os.makedirs(yolo_dir, exist_ok=True)

images_dir = os.path.join(cleaned_dir, "images")
cleaned_json_path = os.path.join(cleaned_dir, "cleaned_coco.json")
class_mapping_csv = os.path.join(cleaned_dir, "class_mapping.csv")

# Train/val folders
train_img_dir = os.path.join(yolo_dir, "train", "images")
train_label_dir = os.path.join(yolo_dir, "train", "labels")
val_img_dir = os.path.join(yolo_dir, "val", "images")
val_label_dir = os.path.join(yolo_dir, "val", "labels")

# ------------------------------------------------------------
# Clear old folders to avoid duplicates from the previous runs
# -------------------------------------------------------------
for folder in [train_img_dir, train_label_dir, val_img_dir, val_label_dir]:
    if os.path.exists(folder):
        shutil.rmtree(folder)
    os.makedirs(folder, exist_ok=True)

# -----------------------------
# Load cleaned COCO JSON
# -----------------------------
with open(cleaned_json_path) as f:
    coco = json.load(f)

# Build image_id -> list of annotations
image_id_to_ann = {}
for ann in coco["annotations"]:
    image_id_to_ann.setdefault(ann["image_id"], []).append(ann)

# ----------------------------------------
# Split images into train (80%) and val (20%)
# ----------------------------------------
all_images = coco["images"]
random.shuffle(all_images)
split_idx = int(len(all_images) * 0.8)
train_images = all_images[:split_idx]
val_images = all_images[split_idx:]

# ----------------------------------------
# Function to convert COCO bbox to YOLO format
# ----------------------------------------
def coco_to_yolo_bbox(bbox, img_w, img_h):
    """
    bbox: [x_min, y_min, width, height]
    returns: [x_center, y_center, width, height] normalized 0-1
    """
    x, y, w, h = bbox
    x_center = (x + w / 2) / img_w
    y_center = (y + h / 2) / img_h
    w_norm = w / img_w
    h_norm = h / img_h
    return [x_center, y_center, w_norm, h_norm]

# -----------------------------
# Process images and generate YOLO labels
# -----------------------------
for subset, images_list, img_dst_dir, label_dst_dir in [
    ("train", train_images, train_img_dir, train_label_dir),
    ("val", val_images, val_img_dir, val_label_dir)
]:
    print(f"Processing {subset} set ({len(images_list)} images)...")

    for img in images_list:
        img_id = img["id"]
        img_file = img["file_name"]
        img_path = os.path.join(images_dir, img_file)

        # Copy image to YOLO folder
        copy2(img_path, os.path.join(img_dst_dir, img_file))

        # Read image size
        with Image.open(img_path) as im:
            img_w, img_h = im.size

        # Prepare YOLO label file
        label_file = os.path.splitext(img_file)[0] + ".txt"
        dst_label = os.path.join(label_dst_dir, label_file)

        # Write annotations in YOLO format
        with open(dst_label, "w") as f:
            for ann in image_id_to_ann.get(img_id, []):
                cls_id = ann["category_id"]  # already mapped 0-4
                x_c, y_c, w_n, h_n = coco_to_yolo_bbox(ann["bbox"], img_w, img_h)
                f.write(f"{cls_id} {x_c} {y_c} {w_n} {h_n}\n")

# -----------------------------
# Copy class_mapping.csv
# -----------------------------
if os.path.exists(class_mapping_csv):
    copy2(class_mapping_csv, os.path.join(yolo_dir, "class_mapping.csv"))
    print("class_mapping.csv copied to YOLO folder")

# -----------------------------
# Generate data.yaml for YOLO
# -----------------------------
data_yaml = {
    'train': os.path.join(yolo_dir, "train", "images"),
    'val': os.path.join(yolo_dir, "val", "images"),
    'nc': 5,
    'names': ["potted plant", "chair", "cup", "vase", "book"]
}

yaml_path = os.path.join(yolo_dir, "data.yaml")
with open(yaml_path, "w") as f:
    yaml.dump(data_yaml, f)

print(f"YOLO dataset ready in '{yolo_dir}'")
print(f"Train images: {len(train_images)}, Val images: {len(val_images)}")
print(f"Data YAML: {yaml_path}")

# --------------------------------------------------------------------------
# Save CSV file recording the number of labels (annotations) for each image
# --------------------------------------------------------------------------

splits = ["train", "val"]  # your dataset splits

for split in splits:
    images_dir = os.path.join(yolo_dir, split, "images")
    labels_dir = os.path.join(yolo_dir, split, "labels")  # YOLO label txt files
    output_csv = os.path.join(yolo_dir, f"{split}_labels_count.csv")

    image_files = [f for f in os.listdir(images_dir) if f.endswith((".jpg", ".png"))]
    data = []

    for image_file in image_files:
        label_file = os.path.splitext(image_file)[0] + ".txt"
        label_path = os.path.join(labels_dir, label_file)

        if os.path.exists(label_path):
            with open(label_path, "r") as f:
                num_labels = sum(1 for line in f if line.strip())
        else:
            num_labels = 0

        data.append([image_file, num_labels])

    # Save CSV
    with open(output_csv, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["image_id", "num_labels"])
        writer.writerows(data)

    print(f"{split} CSV saved as {output_csv}")